# A Structured Path for Learning AI / ML Research & Engineering

I'm developing this course to contain **everything you need** to:

* Join elite labs like **OpenAI**, **Google**, or **MIT**
* Independently publish **groundbreaking open-source research**
* Build **world-class models**


## üì∫ [My YouTube Videos](https://www.youtube.com/channel/UC7XJj9pv_11a11FUxCMz15g)

Follow along with course walkthroughs, video tutorials, and explanations.



## ‚ö° Speedruns

For advanced learners, check out the [Speedruns](speedruns):
Research / engineering / optimization challenges that help you:

* Contribute to open-source
* Build real skills by doing


# üìö Course

üìò [Introduction & Motivation](001%20Introduction%20%26%20Motivation)


## üêç Don't Know Python?

Start with the [Beginner Python Course](beginner-course) to get up to speed.

### üóÇÔ∏è Tip

Make a copy of the notebooks:
**Open notebook ‚Üí File ‚Üí Save a copy to Dive**

# Fundamentals (Most important)

- PyTorch Fundamentals: From Linear Layers & Weight Intuition to LayerNorm, Variance, and Custom ML Blocks - [Google Colab](https://colab.research.google.com/drive/1Mk37B4ISgmhTNDEVTCB3R_Fwp5zEEqVS?usp=sharing) - [YouTube](https://youtu.be/QtlDV2r1ryE) - [Bilibili](https://www.bilibili.com/video/BV17LGczLED3/)

- Code Softmax, Cross-Entropy, and Gradients ‚Äî From Scratch (No Torch) (_In development_) - [Googe Colab](https://colab.research.google.com/drive/1eCRAS6c0Fdy3PBitC2aztSDzp_CoSa_W?usp=sharing)

    ## Backpropagation

- Chain Rule & Backpropagation From Scratch [Google Colab](https://colab.research.google.com/drive/1wcNWayyiB8i4fjKEYmsiIuJmGEN5lTQK?usp=sharing)

    ## Matrix Multiplication

- Comparing MatMul: PyTorch Native vs Tiling vs Quantization (_In development_) - [Google Colab](https://colab.research.google.com/drive/1a_tkXxZ0gt3gFd52IP25bwrVvL8Cenyu?usp=sharing)

- Make Matrix Multiply 3x Faster by Padding Size to Power of 2 - [Google Colab](https://colab.research.google.com/drive/1VKIQS5ocefunYkoE-8uFz_0_xOtkBelG?usp=sharing)

- How Matrix Shape Affects Performance on Nvidia T4 Tensor Cores - _(in development)_ - [Google Colab](https://colab.research.google.com/drive/1eiWkfbrNv2GW7kDMty1jf_xzrEuqHOrO?usp=sharing)

- TODO: how to optimize matmuls on specific GPUs

    ## Training LLMs From Scratch
    
- Experimenting With Small Character-Level LLM:
Hyperparameters, Optimization, and Model Scaling - [Paper](https://drive.google.com/file/d/1sXN-c-L7z3ku29N4QVp6mAP7ZuX7B7Xf/view?usp=sharing) - [Google Colab](https://colab.research.google.com/drive/11bc71DzTe95XDq6IRbJPQtAy_pVD9fhC?usp=sharing)

# High Performance on Hopper GPUs (H100, H200, H800)

- TMA (Tensor Memory Accelerator) alignment for fast memory on Hopper GPUs (DeepSeek's speed) - [Google Colab](https://colab.research.google.com/drive/1F6CNQND2F9a4yLLYqorNAkKEzVxQurCa?usp=sharing)

- High-Performance GPU Matrix Multiplication on H800, H100 & H200 from Scratch - [Google Colab](https://colab.research.google.com/drive/1zxrSNFySwuNycT30Huy3bjxvoEjHbrMa?usp=sharing)

# Fun experiments

- Looking for patterns in trained neural network weights - [Google Colab](https://colab.research.google.com/drive/1P7KreHpJcZL4vjDrRqd69eqsEFYl_2Oa?usp=sharing) - [Preview PDF Analysis](https://file.notion.so/f/f/795d9b1f-4854-4c8d-8295-2ca702b9d498/439a9db5-3835-4ef6-82df-d2576aed18a2/Looking_for_patterns_in_trained_neural_network_weights.pdf?table=block&id=22d7982f-d437-80e7-96f4-e1d80912ff49&spaceId=795d9b1f-4854-4c8d-8295-2ca702b9d498&expirationTimestamp=1752271200000&signature=fV4tMNJ0SwedGljjU3-guwNywDRrhy2XvFrDPWMgqBI&downloadName=Looking+for+patterns+in+trained+neural+network+weights.pdf) _In development_