A structured path for learning AI / LM research with my [YouTube videos](https://www.youtube.com/channel/UC7XJj9pv_11a11FUxCMz15g).

I'm developing this course to contain everything you need to join elite labs like OpenAI, Google, or MIT, or to independently publish groundbreaking open-source research and build world-class models.

[Introduction & Motivation](001%20Introduction%20%26%20Motivation)

If you don't know Python, [click here](beginner-course) for the Python course.

I recommend making a copies of the notebooks (Open notebook -> File -> Save copy to drive).

---

# Fundamentals (Most important)

- PyTorch Fundamentals: From Linear Layers & Weight Intuition to LayerNorm, Variance, and Custom ML Blocks - [Google Colab](https://colab.research.google.com/drive/1Mk37B4ISgmhTNDEVTCB3R_Fwp5zEEqVS?usp=sharing) - [YouTube](https://youtu.be/QtlDV2r1ryE) - [Bilibili](https://www.bilibili.com/video/BV17LGczLED3/)

- Code Softmax, Cross-Entropy, and Gradients â€” From Scratch (No Torch) (_In development_) - [Googe Colab](https://colab.research.google.com/drive/1eCRAS6c0Fdy3PBitC2aztSDzp_CoSa_W?usp=sharing)

    ## Backpropagation

- Backpropagation from scratch (_In development_) - [Google Colab]

    ## Matrix Multiplication

- Comparing MatMul: PyTorch Native vs Tiling vs Quantization (_In development_) - [Google Colab](https://colab.research.google.com/drive/1a_tkXxZ0gt3gFd52IP25bwrVvL8Cenyu?usp=sharing)

- TODO: how to optimize matmuls on specific GPUs

# High Performance on Hopper GPUs (H100, H200, H800)

- TMA (Tensor Memory Accelerator) alignment for fast memory on Hopper GPUs (DeepSeek's speed) - [Google Colab](https://colab.research.google.com/drive/1F6CNQND2F9a4yLLYqorNAkKEzVxQurCa?usp=sharing)

- High-Performance GPU Matrix Multiplication on H800, H100 & H200 from Scratch - [Google Colab](https://colab.research.google.com/drive/1zxrSNFySwuNycT30Huy3bjxvoEjHbrMa?usp=sharing)

