# A Structured Path for Learning AI / ML Research & Engineering

I'm developing this course to contain **everything you need** to:

* Join elite labs like **OpenAI**, **Google**, or **MIT**
* Independently publish **groundbreaking open-source research**
* Build **world-class models**


## üì∫ [My YouTube Videos](https://www.youtube.com/channel/UC7XJj9pv_11a11FUxCMz15g)

Follow along with course walkthroughs, video tutorials, and explanations.



## ‚ö° Speedruns

For advanced learners, check out the [Speedruns](speedruns):
Research / engineering / optimization challenges that help you:

* Contribute to open-source
* Build real skills by doing


# üìö Course

üìò [Introduction & Motivation](001%20Introduction%20%26%20Motivation)


## üêç Don't Know Python?

Start with the [Beginner Python Course](beginner-course) to get up to speed.

### üóÇÔ∏è Tip

Make a copy of the notebooks:
**Open notebook ‚Üí File ‚Üí Save a copy to Dive**

# Fundamentals (Most important)

- PyTorch Fundamentals: From Linear Layers & Weight Intuition to LayerNorm, Variance, and Custom ML Blocks - [Google Colab](https://colab.research.google.com/drive/1Mk37B4ISgmhTNDEVTCB3R_Fwp5zEEqVS?usp=sharing) - [YouTube](https://youtu.be/QtlDV2r1ryE) - [Bilibili](https://www.bilibili.com/video/BV17LGczLED3/)

- Code Softmax, Cross-Entropy, and Gradients ‚Äî From Scratch (No Torch) (_In development_) - [Googe Colab](https://colab.research.google.com/drive/1eCRAS6c0Fdy3PBitC2aztSDzp_CoSa_W?usp=sharing)

    ## Backpropagation

- Chain Rule & Backpropagation From Scratch [Google Colab](https://colab.research.google.com/drive/1wcNWayyiB8i4fjKEYmsiIuJmGEN5lTQK?usp=sharing)

    ## Matrix Multiplication

- Comparing MatMul: PyTorch Native vs Tiling vs Quantization (_In development_) - [Google Colab](https://colab.research.google.com/drive/1a_tkXxZ0gt3gFd52IP25bwrVvL8Cenyu?usp=sharing)

- Make Matrix Multiply 3x Faster by Padding Size to Power of 2 - [Google Colab](https://colab.research.google.com/drive/1VKIQS5ocefunYkoE-8uFz_0_xOtkBelG?usp=sharing)

- TODO: how to optimize matmuls on specific GPUs

# High Performance on Hopper GPUs (H100, H200, H800)

- TMA (Tensor Memory Accelerator) alignment for fast memory on Hopper GPUs (DeepSeek's speed) - [Google Colab](https://colab.research.google.com/drive/1F6CNQND2F9a4yLLYqorNAkKEzVxQurCa?usp=sharing)

- High-Performance GPU Matrix Multiplication on H800, H100 & H200 from Scratch - [Google Colab](https://colab.research.google.com/drive/1zxrSNFySwuNycT30Huy3bjxvoEjHbrMa?usp=sharing)

